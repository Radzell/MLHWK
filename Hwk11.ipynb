{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import copy\n",
      "import matplotlib.pyplot as plt\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 840
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tanh(x):\n",
      "    return np.tanh(x)\n",
      "\n",
      "def tanh_deriv(x):\n",
      "    return 1.0 - x**2\n",
      "\n",
      "def logistic(x):\n",
      "    return x\n",
      "\n",
      "def logistic_derivative(x):\n",
      "    return 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 841
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetwork:\n",
      "    \n",
      "    def __init__(self, layers,w=None,activation='tanh'):\n",
      "        \"\"\"\n",
      "        :param layers: A list containing the number of units in each layer. Should be at least two values\n",
      "        :param activation: The activation function to be used. Can be \"logistic\" or \"tanh\"\n",
      "        \"\"\"\n",
      "        self.ident = activation\n",
      "        if activation == 'ident':\n",
      "            self.activation = logistic\n",
      "            self.activation_deriv = logistic_derivative\n",
      "        elif activation == 'tanh':\n",
      "            self.activation = tanh\n",
      "            self.activation_deriv = tanh_deriv\n",
      "        if(w is None):\n",
      "            self.weights = []\n",
      "            for i in range(1, len(layers) - 1):\n",
      "                self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i] + 1))-1)*0.25)\n",
      "            self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)\n",
      "        else:\n",
      "            self.weights = w\n",
      "        self.L = len(self.weights)\n",
      "    def train(self,X,y,max_itr=100,alpha=1.1,beta=0.8,eta=.01):\n",
      "        Einlist = []\n",
      "        weightlist=[]\n",
      "        Gl,Ein = self.fitBeta(X,y)\n",
      "        for i in range(max_itr):\n",
      "            Gl,Ein = self.fitBeta(X,y)\n",
      "            Vt=np.multiply(Gl,-1)\n",
      "            w = self.weights\n",
      "            self.weights +=eta*Vt\n",
      "            #print self.weights\n",
      "            Glpt,Einpot = self.fitBeta(X,y)\n",
      "            if(Einpot<Ein):\n",
      "                eta = alpha*eta\n",
      "                Einlist.append(Einpot)\n",
      "                weightlist.append(self.weights)\n",
      "            else:\n",
      "                self.weights=w\n",
      "                eta=beta*eta\n",
      "        return Einlist,weightlist\n",
      "    def fitBeta(self, X, y):\n",
      "        Ein = 0\n",
      "        N = len(X)\n",
      "        Gl = copy.deepcopy(self.weights)        \n",
      "        for layer in range(len(Gl)):\n",
      "            for i in range(len(Gl[layer])):\n",
      "                for j in range(len(Gl[layer][i])):\n",
      "                    Gl[layer][i][j] = 0\n",
      "        for i in range(len(X)):\n",
      "            x = X[i]\n",
      "            xl,sl=self.forward_prop(x)\n",
      "            deltas = self.back_prop(xl,sl) \n",
      "            Ein =Ein+ (1.0/(4.0*N))*(xl[-1]-y[i])**2\n",
      "            for l in range(self.L):\n",
      "                d = np.atleast_2d(deltas[l])\n",
      "                GlXn = np.dot(2*(xl[-1][0]-y[i]),np.dot(xl[l],d.T))\n",
      "                Gl[l]=Gl[l]+(1.0/(4.0*N))*(GlXn/(N))\n",
      "        return Gl,Ein\n",
      "    def back_prop(self,xl,sl):\n",
      "        deltas = [None]*self.L\n",
      "        deltas[self.L-1]=self.activation_deriv(xl[-1])\n",
      "        for i in reversed(range(self.L-1)):\n",
      "            w=np.array(self.weights[i+1][1:])\n",
      "            if not self.ident=='ident':\n",
      "                d=np.atleast_2d(deltas[i+1])\n",
      "                deltas[i]=np.dot(np.diagflat(self.activation_deriv(xl[i+1][1:])),np.atleast_2d(w.dot(d)))\n",
      "            else:\n",
      "                deltas[i]=w.dot(deltas[i+1])\n",
      "        return deltas\n",
      "    def forward_prop(self,X):\n",
      "        #adding bias to X\n",
      "        temp = np.array(X)\n",
      "        xl = [temp]\n",
      "        sl=[]\n",
      "        for i in range(self.L):\n",
      "            w=np.array(self.weights[i])\n",
      "            x=xl[i]\n",
      "            sl.append(w.T.dot(x))\n",
      "            temp = sl[-1]\n",
      "            temp=self.activation(temp)\n",
      "            temp=np.insert(temp,0,1,axis=0)\n",
      "            xl.append(temp)\n",
      "        xl[-1]=np.delete(xl[-1],0)\n",
      "        return xl,sl\n",
      "    def calc_err(self,X,Y,lamb):\n",
      "        err = 0\n",
      "        for i in range(len(X)):\n",
      "            x = X[i]\n",
      "            y=Y[i]\n",
      "            xl,sl=self.forward_prop(x)\n",
      "            h=xl[-1]\n",
      "            err += (h-y)**2 + (lamb * sum([w**2 for l in self.weights for r in l for w in r]))\n",
      "        err /= (4.0*len(X))\n",
      "        return err\n",
      "    def numgrad(self,X,y,epsilon=0.0001,lamb=0):\n",
      "        gradient = copy.deepcopy(self.weights)        \n",
      "        for layer in range(len(gradient)):\n",
      "            for i in range(len(gradient[layer])):\n",
      "                for j in range(len(gradient[layer][i])):\n",
      "                    gradient[layer][i][j] = 0\n",
      "        for layer in range(len(self.weights)):\n",
      "            for i in range(len(self.weights[layer])):\n",
      "                for j in range(len(self.weights[layer][i])):\n",
      "                    #Perturb weight\n",
      "                    self.weights[layer][i][j] += epsilon\n",
      "                    f1 = self.calc_err(X,y,lamb)\n",
      "                    self.weights[layer][i][j] -= 2*epsilon\n",
      "                    f2 = self.calc_err(X,y,lamb)\n",
      "                    #Return weight to original state\n",
      "                    self.weights[layer][i][j] += epsilon\n",
      "                    gradient[layer][i][j] = (f1 - f2) / (2 * epsilon) + float(2*lamb*self.weights[layer][i][j])\n",
      "        return gradient"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 842
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w=[\n",
      "                [\n",
      "                        [0.25, 0.25],\n",
      "                        [0.25, 0.25]\n",
      "                ],\n",
      "                [\n",
      "                        [0.25],\n",
      "                        [0.25],\n",
      "                        [0.25]\n",
      "                ]\n",
      "]\n",
      "X = [\n",
      "                 [       \n",
      "                         [1],\n",
      "                         [1]\n",
      "                 ]\n",
      "     \n",
      "     ]\n",
      "y= [1]\n",
      "nn = NeuralNetwork([2,2,1],w=w ,activation='tanh')\n",
      "Elist,wlist=nn.train(X,y,max_itr=(2*10**3))\n",
      "plt.plot(Elist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 849,
       "text": [
        "[<matplotlib.lines.Line2D at 0x6600d90>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSOUvIpKDVP4iIjlI5S8ikoP+H6n8YsgiMj4DAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x3461ed0>"
       ]
      }
     ],
     "prompt_number": 849
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w=[\n",
      "                [\n",
      "                        [0.1, 0.2], \n",
      "                        [0.3, 0.4],\n",
      "                ],\n",
      "                [\n",
      "                        [0.2],\n",
      "                        [1],\n",
      "                        [-3]\n",
      "                ],\n",
      "                [\n",
      "                        [1],\n",
      "                        [2]\n",
      "                ]\n",
      "]\n",
      "X = [\n",
      "                 [       \n",
      "                         [1],\n",
      "                         [2]\n",
      "                 ]\n",
      "     \n",
      "     ]\n",
      "y= [1]\n",
      "nn = NeuralNetwork([2,2,1],w=w ,activation='tanh')\n",
      "g_act,Ein = np.array(nn.fitBeta(X,y))\n",
      "g_num =  np.array(nn.numgrad(X,y))\n",
      "print 'act {0}'.format(g_act)\n",
      "print 'num {0}'.format(g_num)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "act [array([[-0.11020959,  0.21875996],\n",
        "       [-0.22041919,  0.43751991]]), array([[-0.17362962],\n",
        "       [-0.10493615],\n",
        "       [-0.1322353 ]]), array([[-0.46371609],\n",
        "       [ 0.41806123]])]\n",
        "num [ [[array([-0.11020959]), array([ 0.21875996])], [array([-0.22041918]), array([ 0.43751998])]]\n",
        " [[array([-0.17362962])], [array([-0.10493615])], [array([-0.1322353])]]\n",
        " [[array([-0.46371609])], [array([ 0.41806123])]]]\n"
       ]
      }
     ],
     "prompt_number": 844
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w=[\n",
      "                [\n",
      "                        [0.25, 0.25],\n",
      "                        [0.25, 0.25]\n",
      "                ],\n",
      "                [\n",
      "                        [0.25],\n",
      "                        [0.25],\n",
      "                        [0.25]\n",
      "                ]\n",
      "]\n",
      "X = [\n",
      "                 [       \n",
      "                         [1],\n",
      "                         [1]\n",
      "                 ]\n",
      "     \n",
      "     ]\n",
      "y= [1]\n",
      "nn = NeuralNetwork([2,2,1],w=w ,activation='tanh')\n",
      "nn.fitBeta(X,y)\n",
      "nn.numgrad(X,y)\n",
      "g_act,Ein = np.array(nn.fitBeta(X,y))\n",
      "g_num =  np.array(nn.numgrad(X,y))\n",
      "print 'Ein {0}'.format(Ein)\n",
      "print 'act {0}'.format(g_act)\n",
      "print 'num {0}'.format(g_num)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ein [ 0.07642709]\n",
        "act [array([[-0.04348936, -0.04348936],\n",
        "       [-0.04348936, -0.04348936]]), array([[-0.22119392],\n",
        "       [-0.1022175 ],\n",
        "       [-0.1022175 ]])]\n",
        "num [ [[array([-0.04348936]), array([-0.04348936])], [array([-0.04348936]), array([-0.04348936])]]\n",
        " [[array([-0.22119392])], [array([-0.1022175])], [array([-0.1022175])]]]\n"
       ]
      }
     ],
     "prompt_number": 845
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w=[\n",
      "                [\n",
      "                        [0.25, 0.25],\n",
      "                        [0.25, 0.25]\n",
      "                ],\n",
      "                [\n",
      "                        [0.25],\n",
      "                        [0.25],\n",
      "                        [0.25]\n",
      "                ]\n",
      "]\n",
      "X = [\n",
      "                 [       \n",
      "                         [1],\n",
      "                         [1]\n",
      "                 ]\n",
      "     \n",
      "     ]\n",
      "y= [1]\n",
      "nn = NeuralNetwork([2,2,1],w=w ,activation='ident')\n",
      "nn.fitBeta(X,y)\n",
      "nn.numgrad(X,y)\n",
      "g_act,Ein = np.array(nn.fitBeta(X,y))\n",
      "g_num =  np.array(nn.numgrad(X,y))\n",
      "print 'Ein {0}'.format(Ein)\n",
      "print 'act {0}'.format(g_act)\n",
      "print 'num {0}'.format(g_num)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ein [ 0.0625]\n",
        "act [array([[-0.0625, -0.0625],\n",
        "       [-0.0625, -0.0625]]), array([[-0.25 ],\n",
        "       [-0.125],\n",
        "       [-0.125]])]\n",
        "num [ [[array([-0.0625]), array([-0.0625])], [array([-0.0625]), array([-0.0625])]]\n",
        " [[array([-0.25])], [array([-0.125])], [array([-0.125])]]]\n"
       ]
      }
     ],
     "prompt_number": 846
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[ 0.30570837]\n",
      "[array([[-0.17395745, -0.17395745],\n",
      "       [-0.17395745, -0.17395745]]), array([[-0.88477567],\n",
      "       [-0.40887002],\n",
      "       [-0.40887002]])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 847,
       "text": [
        "[array([[-0.17395745, -0.17395745],\n",
        "       [-0.17395745, -0.17395745]]),\n",
        " array([[-0.88477567],\n",
        "       [-0.40887002],\n",
        "       [-0.40887002]])]"
       ]
      }
     ],
     "prompt_number": 847
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 847
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 847
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 847
    }
   ],
   "metadata": {}
  }
 ]
}