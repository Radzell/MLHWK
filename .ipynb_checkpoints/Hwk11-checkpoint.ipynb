{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import copy\n",
      "import matplotlib.pyplot as plt\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 840
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tanh(x):\n",
      "    return np.tanh(x)\n",
      "\n",
      "def tanh_deriv(x):\n",
      "    return 1.0 - x**2\n",
      "\n",
      "def logistic(x):\n",
      "    return x\n",
      "\n",
      "def logistic_derivative(x):\n",
      "    return 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 841
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetwork:\n",
      "    \n",
      "    def __init__(self, layers,w=None,activation='tanh'):\n",
      "        \"\"\"\n",
      "        :param layers: A list containing the number of units in each layer. Should be at least two values\n",
      "        :param activation: The activation function to be used. Can be \"logistic\" or \"tanh\"\n",
      "        \"\"\"\n",
      "        self.ident = activation\n",
      "        if activation == 'ident':\n",
      "            self.activation = logistic\n",
      "            self.activation_deriv = logistic_derivative\n",
      "        elif activation == 'tanh':\n",
      "            self.activation = tanh\n",
      "            self.activation_deriv = tanh_deriv\n",
      "        if(w is None):\n",
      "            self.weights = []\n",
      "            for i in range(1, len(layers) - 1):\n",
      "                self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i] + 1))-1)*0.25)\n",
      "            self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)\n",
      "        else:\n",
      "            self.weights = w\n",
      "        self.L = len(self.weights)\n",
      "    def train(self,X,y,max_itr=100,alpha=1.1,beta=0.8,eta=.01):\n",
      "        Einlist = []\n",
      "        weightlist=[]\n",
      "        Gl,Ein = self.fitBeta(X,y)\n",
      "        for i in range(max_itr):\n",
      "            Gl,Ein = self.fitBeta(X,y)\n",
      "            Vt=np.multiply(Gl,-1)\n",
      "            w = self.weights\n",
      "            self.weights +=eta*Vt\n",
      "            #print self.weights\n",
      "            Glpt,Einpot = self.fitBeta(X,y)\n",
      "            if(Einpot<Ein):\n",
      "                eta = alpha*eta\n",
      "                Einlist.append(Einpot)\n",
      "                weightlist.append(self.weights)\n",
      "            else:\n",
      "                self.weights=w\n",
      "                eta=beta*eta\n",
      "        return Einlist,weightlist\n",
      "    def fitBeta(self, X, y):\n",
      "        Ein = 0\n",
      "        N = len(X)\n",
      "        Gl = copy.deepcopy(self.weights)        \n",
      "        for layer in range(len(Gl)):\n",
      "            for i in range(len(Gl[layer])):\n",
      "                for j in range(len(Gl[layer][i])):\n",
      "                    Gl[layer][i][j] = 0\n",
      "        for i in range(len(X)):\n",
      "            x = X[i]\n",
      "            xl,sl=self.forward_prop(x)\n",
      "            deltas = self.back_prop(xl,sl) \n",
      "            Ein =Ein+ (1.0/(4.0*N))*(xl[-1]-y[i])**2\n",
      "            for l in range(self.L):\n",
      "                d = np.atleast_2d(deltas[l])\n",
      "                GlXn = np.dot(2*(xl[-1][0]-y[i]),np.dot(xl[l],d.T))\n",
      "                Gl[l]=Gl[l]+(1.0/(4.0*N))*(GlXn/(N))\n",
      "        return Gl,Ein\n",
      "    def back_prop(self,xl,sl):\n",
      "        deltas = [None]*self.L\n",
      "        deltas[self.L-1]=self.activation_deriv(xl[-1])\n",
      "        for i in reversed(range(self.L-1)):\n",
      "            w=np.array(self.weights[i+1][1:])\n",
      "            if not self.ident=='ident':\n",
      "                d=np.atleast_2d(deltas[i+1])\n",
      "                deltas[i]=np.dot(np.diagflat(self.activation_deriv(xl[i+1][1:])),np.atleast_2d(w.dot(d)))\n",
      "            else:\n",
      "                deltas[i]=w.dot(deltas[i+1])\n",
      "        return deltas\n",
      "    def forward_prop(self,X):\n",
      "        #adding bias to X\n",
      "        temp = np.array(X)\n",
      "        xl = [temp]\n",
      "        sl=[]\n",
      "        for i in range(self.L):\n",
      "            w=np.array(self.weights[i])\n",
      "            x=xl[i]\n",
      "            sl.append(w.T.dot(x))\n",
      "            temp = sl[-1]\n",
      "            temp=self.activation(temp)\n",
      "            temp=np.insert(temp,0,1,axis=0)\n",
      "            xl.append(temp)\n",
      "        xl[-1]=np.delete(xl[-1],0)\n",
      "        return xl,sl\n",
      "    def calc_err(self,X,Y,lamb):\n",
      "        err = 0\n",
      "        for i in range(len(X)):\n",
      "            x = X[i]\n",
      "            y=Y[i]\n",
      "            xl,sl=self.forward_prop(x)\n",
      "            h=xl[-1]\n",
      "            err += (h-y)**2 + (lamb * sum([w**2 for l in self.weights for r in l for w in r]))\n",
      "        err /= (4.0*len(X))\n",
      "        return err\n",
      "    def numgrad(self,X,y,epsilon=0.0001,lamb=0):\n",
      "        gradient = copy.deepcopy(self.weights)        \n",
      "        for layer in range(len(gradient)):\n",
      "            for i in range(len(gradient[layer])):\n",
      "                for j in range(len(gradient[layer][i])):\n",
      "                    gradient[layer][i][j] = 0\n",
      "        for layer in range(len(self.weights)):\n",
      "            for i in range(len(self.weights[layer])):\n",
      "                for j in range(len(self.weights[layer][i])):\n",
      "                    #Perturb weight\n",
      "                    self.weights[layer][i][j] += epsilon\n",
      "                    f1 = self.calc_err(X,y,lamb)\n",
      "                    self.weights[layer][i][j] -= 2*epsilon\n",
      "                    f2 = self.calc_err(X,y,lamb)\n",
      "                    #Return weight to original state\n",
      "                    self.weights[layer][i][j] += epsilon\n",
      "                    gradient[layer][i][j] = (f1 - f2) / (2 * epsilon) + float(2*lamb*self.weights[layer][i][j])\n",
      "        return gradient"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 842
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w=[\n",
      "                [\n",
      "                        [0.25, 0.25],\n",
      "                        [0.25, 0.25]\n",
      "                ],\n",
      "                [\n",
      "                        [0.25],\n",
      "                        [0.25],\n",
      "                        [0.25]\n",
      "                ]\n",
      "]\n",
      "X = [\n",
      "                 [       \n",
      "                         [1],\n",
      "                         [1]\n",
      "                 ]\n",
      "     \n",
      "     ]\n",
      "y= [1]\n",
      "nn = NeuralNetwork([2,2,1],w=w ,activation='tanh')\n",
      "Elist,wlist=nn.train(X,y,max_itr=(2*10**3))\n",
      "plt.plot(Elist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w=[\n",
      "                [\n",
      "                        [0.1, 0.2], \n",
      "                        [0.3, 0.4],\n",
      "                ],\n",
      "                [\n",
      "                        [0.2],\n",
      "                        [1],\n",
      "                        [-3]\n",
      "                ],\n",
      "                [\n",
      "                        [1],\n",
      "                        [2]\n",
      "                ]\n",
      "]\n",
      "X = [\n",
      "                 [       \n",
      "                         [1],\n",
      "                         [2]\n",
      "                 ]\n",
      "     \n",
      "     ]\n",
      "y= [1]\n",
      "nn = NeuralNetwork([2,2,1],w=w ,activation='tanh')\n",
      "g_act,Ein = np.array(nn.fitBeta(X,y))\n",
      "g_num =  np.array(nn.numgrad(X,y))\n",
      "print 'act {0}'.format(g_act)\n",
      "print 'num {0}'.format(g_num)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "act [array([[-0.11020959,  0.21875996],\n",
        "       [-0.22041919,  0.43751991]]), array([[-0.17362962],\n",
        "       [-0.10493615],\n",
        "       [-0.1322353 ]]), array([[-0.46371609],\n",
        "       [ 0.41806123]])]\n",
        "num [ [[array([-0.11020959]), array([ 0.21875996])], [array([-0.22041918]), array([ 0.43751998])]]\n",
        " [[array([-0.17362962])], [array([-0.10493615])], [array([-0.1322353])]]\n",
        " [[array([-0.46371609])], [array([ 0.41806123])]]]\n"
       ]
      }
     ],
     "prompt_number": 844
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w=[\n",
      "                [\n",
      "                        [0.25, 0.25],\n",
      "                        [0.25, 0.25]\n",
      "                ],\n",
      "                [\n",
      "                        [0.25],\n",
      "                        [0.25],\n",
      "                        [0.25]\n",
      "                ]\n",
      "]\n",
      "X = [\n",
      "                 [       \n",
      "                         [1],\n",
      "                         [1]\n",
      "                 ]\n",
      "     \n",
      "     ]\n",
      "y= [1]\n",
      "nn = NeuralNetwork([2,2,1],w=w ,activation='tanh')\n",
      "nn.fitBeta(X,y)\n",
      "nn.numgrad(X,y)\n",
      "g_act,Ein = np.array(nn.fitBeta(X,y))\n",
      "g_num =  np.array(nn.numgrad(X,y))\n",
      "print 'Ein {0}'.format(Ein)\n",
      "print 'act {0}'.format(g_act)\n",
      "print 'num {0}'.format(g_num)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ein [ 0.07642709]\n",
        "act [array([[-0.04348936, -0.04348936],\n",
        "       [-0.04348936, -0.04348936]]), array([[-0.22119392],\n",
        "       [-0.1022175 ],\n",
        "       [-0.1022175 ]])]\n",
        "num [ [[array([-0.04348936]), array([-0.04348936])], [array([-0.04348936]), array([-0.04348936])]]\n",
        " [[array([-0.22119392])], [array([-0.1022175])], [array([-0.1022175])]]]\n"
       ]
      }
     ],
     "prompt_number": 845
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w=[\n",
      "                [\n",
      "                        [0.25, 0.25],\n",
      "                        [0.25, 0.25]\n",
      "                ],\n",
      "                [\n",
      "                        [0.25],\n",
      "                        [0.25],\n",
      "                        [0.25]\n",
      "                ]\n",
      "]\n",
      "X = [\n",
      "                 [       \n",
      "                         [1],\n",
      "                         [1]\n",
      "                 ]\n",
      "     \n",
      "     ]\n",
      "y= [1]\n",
      "nn = NeuralNetwork([2,2,1],w=w ,activation='ident')\n",
      "nn.fitBeta(X,y)\n",
      "nn.numgrad(X,y)\n",
      "g_act,Ein = np.array(nn.fitBeta(X,y))\n",
      "g_num =  np.array(nn.numgrad(X,y))\n",
      "print 'Ein {0}'.format(Ein)\n",
      "print 'act {0}'.format(g_act)\n",
      "print 'num {0}'.format(g_num)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ein [ 0.0625]\n",
        "act [array([[-0.0625, -0.0625],\n",
        "       [-0.0625, -0.0625]]), array([[-0.25 ],\n",
        "       [-0.125],\n",
        "       [-0.125]])]\n",
        "num [ [[array([-0.0625]), array([-0.0625])], [array([-0.0625]), array([-0.0625])]]\n",
        " [[array([-0.25])], [array([-0.125])], [array([-0.125])]]]\n"
       ]
      }
     ],
     "prompt_number": 846
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[ 0.30570837]\n",
      "[array([[-0.17395745, -0.17395745],\n",
      "       [-0.17395745, -0.17395745]]), array([[-0.88477567],\n",
      "       [-0.40887002],\n",
      "       [-0.40887002]])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 847,
       "text": [
        "[array([[-0.17395745, -0.17395745],\n",
        "       [-0.17395745, -0.17395745]]),\n",
        " array([[-0.88477567],\n",
        "       [-0.40887002],\n",
        "       [-0.40887002]])]"
       ]
      }
     ],
     "prompt_number": 847
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 847
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 847
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 847
    }
   ],
   "metadata": {}
  }
 ]
}